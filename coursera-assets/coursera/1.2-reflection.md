# Reflection

## Key Points

1. **Quality gates prevent bad data propagation**: In production systems, a single corrupted record can cascade through downstream tables, dashboards, and ML models before anyone notices. By placing expectations directly in the pipeline, you catch violations at the point of ingestion rather than discovering them days later in a quarterly business review.
2. **Choose enforcement mode based on data criticality**: Not all data quality issues deserve the same response. A null email address in a marketing table might warrant a warning and a retained row, while a negative transaction amount in a financial pipeline should halt processing entirely. Matching enforcement mode to business impact prevents both data loss from over-aggressive dropping and data corruption from under-enforcement.
3. **Expectations are declarative quality contracts**: Rather than scattering validation logic across multiple scripts and notebooks, expectations define quality rules as part of the table definition itself. This means the contract between data producers and consumers is visible, version-controlled, and automatically enforced -- eliminating the "it worked on my machine" problem for data quality.

## Practical Scenarios

### Scenario 1: Financial Transaction Pipeline

**Situation**: Your team processes millions of payment transactions daily. Downstream fraud detection models and financial reports depend on every record having a valid transaction amount, timestamp, and merchant ID. A recent incident allowed records with null amounts to reach the reporting layer, causing incorrect revenue figures.

**Solution**: Apply strict expectations on the transaction ingestion table: require non-null amounts, positive values, and valid timestamps. Use the "fail" enforcement mode so the pipeline halts immediately if any record violates these constraints. Combine multiple constraints for defense in depth -- check not just for nulls but also for reasonable value ranges. The metrics tracking will automatically count violations, giving the on-call engineer immediate visibility into data quality trends.

### Scenario 2: IoT Sensor Data with Known Noise

**Situation**: Your pipeline ingests temperature readings from thousands of warehouse sensors. Sensors occasionally send readings of -999 or 9999 due to hardware glitches. These outliers should not block the pipeline, but they must not pollute the analytics tables used for climate control optimization.

**Solution**: Define expectations that flag readings outside the physically plausible range (e.g., between -40 and 60 degrees Celsius) and use the "drop" enforcement mode. Invalid rows are silently removed from the output table while the pipeline continues processing valid data. The built-in violation metrics let you monitor which sensors are failing most frequently, feeding back into your hardware maintenance schedule without any custom monitoring code.

## Journal Prompt

Consider a situation where bad data caused a real problem in your work -- an incorrect report, a failed model, or a customer-facing error. At what point in the pipeline could a quality expectation have caught the issue, and what enforcement mode would have been appropriate?

## References

- [Shankar et al., "Moving Fast With Broken Data," arXiv:2303.06094, 2023](https://arxiv.org/abs/2303.06094)
- [Zhou et al., "A Survey on Data Quality Dimensions and Tools for Machine Learning," arXiv:2406.19614, 2024](https://arxiv.org/abs/2406.19614)
