# Reflection

## Key Points

1. **Profiling first then cleaning**: Running summary statistics, null-rate analysis, and value distribution histograms before writing any transformation logic prevents you from building cleaning rules based on assumptions that the data does not actually support.
2. **Exploration reveals hidden data issues**: A column labeled "age" turns out to contain negative values and dates encoded as integers. Without exploratory profiling, a pipeline would silently propagate these values into silver, where they corrupt downstream aggregates for months before anyone notices.
3. **Deduplication is rarely straightforward**: Two records may share the same primary key but carry different timestamps, update flags, or source systems. Deciding which record to keep requires business context, not just a generic `DISTINCT` or `ROW_NUMBER()` call.
4. **Silver enforces type safety and schema**: Casting a string column to a proper timestamp or decimal type at the silver layer means every downstream consumer inherits correct types automatically, eliminating an entire category of bugs where dashboards silently compare strings instead of dates.
5. **Expectations bridge bronze to silver**: Databricks expectations let you declare constraints like "age must be between 0 and 120" directly in the pipeline. Records that violate expectations are quarantined rather than silently dropped, giving the team visibility into data quality without halting production.
6. **Row count comparison catches silent drops**: After every bronze-to-silver transformation, comparing input row count against output row count plus quarantined row count must balance to zero. A discrepancy signals a bug in the transformation logic, such as an unintended filter or a JOIN that fans out rows.
7. **Quality is a spectrum not a binary**: Some records are fully clean, some are usable with caveats, and some must be rejected. Tagging records with a quality score or confidence flag at the silver layer lets gold consumers decide their own tolerance threshold rather than forcing a single pass/fail decision for everyone.

## Practical Scenarios

### Scenario 1: Discovering duplicate patient records across hospital systems

**Situation**: After merging bronze data from three hospital networks, the silver pipeline must unify patient records. Some patients appear in all three systems with slightly different spellings of their name and different medical record numbers.

**Solution**: Profile the data first to quantify the overlap rate and name variation patterns. Then apply a deduplication strategy that uses fuzzy matching on name plus exact matching on date of birth and insurance ID, with an expectations check that flags any patient linked to more than five source records for manual review.

### Scenario 2: Detecting a silent schema drift that drops a column

**Situation**: A vendor quietly renames a column from "transaction_amt" to "txn_amount" in their export. The bronze layer ingests both schemas without error, but the silver transformation references the old column name and silently produces nulls for every new record.

**Solution**: Add a row count and null-rate expectation at the silver boundary. When the null rate for the amount column jumps from 0.1% to 100% overnight, the expectation fires an alert. The team updates the silver mapping, replays from bronze, and adds a schema drift monitor to prevent recurrence.

## Journal Prompt

Consider a dataset you work with regularly. If you ran a full profiling pass on it today, what hidden quality issues do you suspect you would find, and how would surfacing those issues change the way you build transformations against that data?

## References

- [Foidl et al., "Data Pipeline Quality: Influencing Factors, Root Causes of Data-related Issues," arXiv:2309.07067, 2023](https://arxiv.org/abs/2309.07067)
- [Zhou et al., "A Survey on Data Quality Dimensions and Tools for Machine Learning," arXiv:2406.19614, 2024](https://arxiv.org/abs/2406.19614)
