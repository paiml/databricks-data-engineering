# Reflection

## Key Points

1. **Gold tables serve specific business questions**: A gold table named `daily_revenue_by_region` is purpose-built for the finance dashboard. It contains exactly the columns and granularity that dashboard needs, rather than a generic wide table that forces every consumer to re-aggregate at query time.
2. **Business logic belongs in gold not silver**: Currency conversion, fiscal calendar mapping, and customer segmentation rules change with business strategy. Isolating these rules in the gold layer means silver remains a stable, reusable foundation that does not need to be rewritten every time the CFO redefines a reporting region.
3. **Incremental append reduces compute cost**: A daily sales gold table that appends only today's new aggregates processes thousands of rows instead of recomputing millions. Over a year, this difference translates to an order-of-magnitude reduction in cluster hours and cost.
4. **Full recompute guarantees correctness**: When a business rule changes retroactively (such as reclassifying a product category for the entire fiscal year), a full recompute from silver ensures every historical row reflects the updated logic. Incremental append alone would leave stale values in previously written partitions.
5. **Z-ordering optimizes common query patterns**: A gold table frequently filtered by `region` and `product_category` benefits from Z-ordering on those columns, which colocates related data in the same files and allows Databricks to skip irrelevant files entirely, reducing scan time by 10x or more on large tables.
6. **Partitioning is a physical optimization choice**: Partitioning a gold table by date works well when most queries filter on date ranges, but over-partitioning on a high-cardinality column like `customer_id` creates millions of tiny files that degrade read performance. The decision must be driven by actual query patterns, not intuition.
7. **Gold tables are the interface to analytics**: Data analysts, BI tools, and ML feature stores all consume gold tables. Treating gold as a published API with documented schemas, SLAs, and versioning ensures that changes to upstream pipelines do not silently break the reports and models that the business depends on.

## Practical Scenarios

### Scenario 1: Choosing between incremental and full recompute for a KPI table

**Situation**: The executive dashboard displays monthly active users (MAU). Mid-quarter, the product team changes the definition of "active" from "logged in at least once" to "completed at least one transaction." Historical reports must reflect the new definition.

**Solution**: Switch the MAU gold table from incremental append mode to a full recompute that re-evaluates every month against the updated definition in silver. After the backfill completes and is validated, return to incremental mode for future months using the new logic. Document the definition change in the gold table's metadata so consumers understand the discontinuity.

### Scenario 2: Optimizing a slow-running analytics query with Z-ordering

**Situation**: An analyst reports that their daily query against the `order_summary` gold table takes 12 minutes. The table contains 2 billion rows, and the query always filters on `order_date` and `warehouse_id`.

**Solution**: Apply `OPTIMIZE order_summary ZORDER BY (order_date, warehouse_id)`. After Z-ordering, the query engine skips files that do not contain the requested date range and warehouse, reducing the scan from 2 billion rows to under 5 million. The query drops from 12 minutes to under 40 seconds, and a scheduled weekly `OPTIMIZE` keeps the benefit as new data arrives.

## Journal Prompt

Think about a report or dashboard your organization relies on. Where does the business logic that powers it currently live -- in the query layer, in a transformation pipeline, or scattered across both? How would centralizing that logic in a dedicated gold layer change the reliability and maintainability of that report?

## References

- [Rong et al., "Dynamic Data Layout Optimization with Worst-case Guarantees," arXiv:2405.04984, 2024](https://arxiv.org/abs/2405.04984)
- [Rucco et al., "Formalizing ETLT and ELTL Design Patterns," arXiv:2511.03393, 2025](https://arxiv.org/abs/2511.03393)
