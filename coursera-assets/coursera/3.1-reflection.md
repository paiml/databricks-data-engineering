# Reflection

## Key Points

1. **CDC enables real-time inventory tracking**: In production environments such as retail or logistics, Change Data Capture eliminates the need for expensive full-table scans by streaming only the rows that changed. This means your inventory dashboard reflects a supplier shipment within seconds of the warehouse system recording it, rather than waiting for a nightly batch job.
2. **apply_changes() handles merge complexity**: Without DLT's `apply_changes()`, you would write verbose MERGE statements that must account for inserts, updates, deletes, and out-of-order arrivals. In a real-world order management system, a single customer might update their shipping address after placing an order — `apply_changes()` collapses that complexity into a declarative call that automatically sequences events by timestamp.
3. **SCD patterns preserve historical context**: Slowly Changing Dimension Type 2 tracking is essential when auditors or analysts need to answer questions like "What was this product's price category when the customer purchased it last quarter?" By materializing SCD Type 2 tables through `apply_changes()` with history enabled, you maintain a complete audit trail without custom versioning logic.
4. **Orchestration ensures correct execution order**: A complete pipeline may have dozens of tables with complex dependencies — bronze ingestion must finish before silver cleansing, and silver must finish before gold aggregation. DLT's built-in dependency resolution means you declare relationships with `dlt.read()` and the framework topologically sorts execution, preventing the subtle race conditions that plague cron-scheduled scripts.
5. **Event log is the observability backbone**: When a pipeline fails at 3 AM, the first question is always "what changed?" The DLT event log captures every expectation violation, schema evolution event, and data quality metric in a queryable Delta table. In a real production incident, you can join the event log against your data lineage to pinpoint whether the root cause was an upstream schema change or a data quality regression.

## Practical Scenarios

### Scenario 1: Real-Time Inventory Reconciliation

**Situation**: A national retail chain runs 400 stores, each with a local point-of-sale system that records sales, returns, and stock adjustments. The central inventory system must reconcile these changes continuously so that the e-commerce website shows accurate stock levels. Currently, a nightly ETL job causes a 12-hour lag, leading to overselling and customer complaints.

**Solution**: Implement a CDC-driven DLT pipeline that captures change events from each store's database using Debezium connectors feeding into a Kafka topic. The bronze layer ingests raw CDC events with `cloud_files()` from the landing zone. A silver layer applies `apply_changes()` keyed on `(store_id, product_sku)` with sequence-by on the event timestamp, automatically handling late-arriving corrections. The gold layer aggregates current inventory per product across all stores, powering a real-time availability API. DLT expectations at the silver layer enforce that `quantity_on_hand >= 0` and that every event has a valid `store_id`, dropping malformed records to a quarantine table for investigation.

### Scenario 2: Orchestrating a Multi-Source Order Pipeline

**Situation**: A B2B distributor receives orders from three channels — EDI files from enterprise customers, a REST API from the web portal, and manual CSV uploads from the sales team. Each source has different schemas, delivery cadences, and reliability characteristics. The analytics team needs a unified orders fact table updated within 15 minutes of any source delivering data.

**Solution**: Build three separate bronze tables in a single DLT pipeline, each using Auto Loader (`cloud_files()`) pointed at the respective landing zones. Silver layer tables normalize all three sources into a common order schema using `apply_changes()` with `APPLY AS DELETE WHEN operation = 'cancel'` to handle order cancellations. Define expectations that enforce required fields (`order_id NOT NULL`, `customer_id NOT NULL`, `quantity > 0`) and set `ON VIOLATION DROP ROW` so that malformed records do not pollute downstream aggregates. The gold layer joins the unified orders table with customer dimensions to produce the analytics fact table. The entire pipeline is orchestrated as a single DLT pipeline with declared dependencies, ensuring that gold tables never compute against stale silver data.

## Journal Prompt

Think about a data pipeline you have built or maintained where changes to source data caused downstream inconsistencies. How would implementing CDC with `apply_changes()` and DLT expectations have changed your debugging experience during the last incident you investigated? What specific failure mode would the event log have helped you diagnose faster?

## References

- [Andreakis et al., "DBLog: A Watermark Based Change-Data-Capture Framework," arXiv:2010.12597, 2020](https://arxiv.org/abs/2010.12597)
- [Foidl et al., "Data Pipeline Quality: Influencing Factors, Root Causes of Data-related Issues," arXiv:2309.07067, 2023](https://arxiv.org/abs/2309.07067)
