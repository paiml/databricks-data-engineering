# Key Terms

**Change Data Capture (CDC)**: Change Data Capture is a data integration pattern that identifies and tracks row-level changes (inserts, updates, deletes) in source systems, propagating only the incremental modifications to downstream tables rather than reprocessing entire datasets. In Delta Live Tables, CDC is implemented through the apply_changes() function, which processes a stream of change events and automatically merges them into a target table while maintaining correct ordering and deduplication.

**apply_changes()**: apply_changes() is a DLT Python Application Programming Interface (API) function that processes a CDC feed by applying insert, update, and delete operations to a target streaming table based on a specified sequence column for ordering and one or more key columns for matching. It supports both Slowly Changing Dimension (SCD) Type 1 (overwrite current state) and SCD Type 2 (maintain full history with validity timestamps), enabling engineers to build accurate dimension tables directly within a declarative pipeline.

**Slowly Changing Dimension (SCD)**: A Slowly Changing Dimension is a data warehousing pattern for tracking changes to dimension attributes over time, with Type 1 overwriting the existing record to reflect only the current state and Type 2 preserving historical versions by adding new rows with effective date ranges. In DLT, the apply_changes() function supports both SCD types through the stored_as_scd_type parameter, allowing the pipeline to maintain either a current-state snapshot or a complete audit trail of all changes.

**Pipeline Orchestration**: Pipeline orchestration in DLT refers to the framework's automatic resolution of table dependencies, execution ordering, and parallelism across all table definitions in a pipeline. Engineers declare tables and their source relationships, and the DLT runtime builds a Directed Acyclic Graph (DAG) to determine which tables can run concurrently and which must wait for upstream dependencies to complete, eliminating the need for manual scheduling or step-by-step imperative logic.

**Event Log**: The DLT event log is a system-generated Delta table that records detailed metadata about every pipeline run, including expectation pass/fail counts, data quality metrics, table update durations, and cluster utilization. Engineers query the event log programmatically or inspect it through the pipeline UI to monitor pipeline health, diagnose failures, track data quality trends, and set up automated alerts on quality regressions.

## References

- [Andreakis et al., "DBLog: A Watermark Based Change-Data-Capture Framework," arXiv:2010.12597, 2020](https://arxiv.org/abs/2010.12597)
- [Foidl et al., "Data Pipeline Quality: Influencing Factors, Root Causes of Data-related Issues," arXiv:2309.07067, 2023](https://arxiv.org/abs/2309.07067)
