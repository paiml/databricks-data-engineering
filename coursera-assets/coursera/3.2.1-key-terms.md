# Key Terms

**Databricks Workflows**: Databricks Workflows is a managed orchestration service that coordinates the execution of multiple tasks — including DLT pipelines, notebooks, Java Archive (JAR) jobs, and Structured Query Language (SQL) queries — within a single Directed Acyclic Graph (DAG) with dependency management, retry logic, and conditional branching. Workflows enable engineers to compose multi-pipeline architectures where independent DLT pipelines for different domains are sequenced, parallelized, and monitored as a unified production job.

**Unity Catalog**: Unity Catalog is the centralized governance layer for the Databricks Lakehouse that provides a three-level namespace (catalog.schema.table) for organizing data assets, along with fine-grained access controls, data lineage tracking, and auditing across all workspaces. It enables data engineering teams to enforce consistent security policies, track how data flows through DLT pipelines and downstream consumers, and share governed datasets across organizational boundaries.

**Data Governance**: Data governance in the Databricks Lakehouse refers to the set of policies, processes, and tools — anchored by Unity Catalog — that ensure data assets are discoverable, secure, compliant, and quality-controlled throughout their lifecycle. For data engineers, this encompasses column-level access controls, row-level security, data classification tags, lineage tracking from ingestion through transformation, and audit logs that document who accessed or modified data and when.

**Multi-Pipeline Orchestration**: Multi-pipeline orchestration is the practice of decomposing a large data platform into separate, focused DLT pipelines — each handling a specific domain or data source — and coordinating their execution through Databricks Workflows. This architecture improves fault isolation (a failure in one pipeline does not halt unrelated processing), enables independent scaling and development velocity per domain, and allows teams to own their pipelines while maintaining a coherent, end-to-end data flow.

**Real-Time Streaming**: Real-time streaming in Databricks extends beyond DLT's micro-batch incremental processing to include Structured Streaming applications that process events with sub-second latency using triggers like availableNow or continuous processing. While DLT streaming tables handle most incremental workloads declaratively, engineers pursuing real-time use cases — such as fraud detection, live dashboards, or event-driven architectures — leverage Structured Streaming with Delta Lake as both a source and a sink for low-latency data delivery.

## References

- [Subramaniam et al., "Comprehensive and Comprehensible Data Catalogs," arXiv:2103.07532, 2021](https://arxiv.org/abs/2103.07532)
- [Wider et al., "Decentralized Data Governance as Part of a Data Mesh Platform," arXiv:2307.02357, 2023](https://arxiv.org/abs/2307.02357)
